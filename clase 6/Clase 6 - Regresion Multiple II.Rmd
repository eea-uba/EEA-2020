---
title: "Regresión Lineal Múltiple II"
author: "Juan Barriola y Sofía Perini"
date: "10 de Octubre de 2020"
output:
  html_notebook:
    theme: spacelab
    toc: yes
    toc_float: yes
    df_print: paged
---

<style type="text/css">
div.main-container {
  max-width: 1600px;
  margin-left: auto;
  margin-right: auto;
}
</style>

## Diagnóstico y Evaluación de Modelos de Regresión Lineal Múltiple

### Dataset 

Vamos a trabajar con el subconjunto de datos que surgió del trabajo de limpieza que se hizo en la clase de regresión lineal simple, correspondiente al grupo de salarios de los data scientists/analyst, de la encuesta de sueldos en el sector de tecnología en Argenina realizada por SysArmy. El informe, realizado por OpenQube lo pueden ver [acá](https://sueldos.openqube.io/encuesta-sueldos-2020.01/).

Nuestro objetivo es evaluar modelos de regresión lineal múltiple que buscan explicar el sueldo neto de Data Analysts, Data Scientists y Data Engineers en Argentina.

Es decir, evaluamos los siguientes modelos para el salario neto:

$salarioNeto = \beta_0 +\beta_1X_1+\beta_2X_2+...+\epsilon$

```{r, warning=F, message=F}
library(tidyverse)
library(tidymodels)
library(gridExtra)
```

#### Levantamos Dataset y seleccionamos variables de interés

```{r}
encuesta <- read_csv("../Fuentes/encuesta_RLM_limpia.csv")
df <- encuesta %>%
  select(me_identifico, edad, donde_estas_trabajando, anos_de_experiencia, anos_en_la_empresa_actual, anos_en_el_puesto_actual, gente_a_cargo, trabajo_de, nivel_de_estudios_alcanzado, estado, salario_bruto, salario_neto) 
# agrego columna de nivel educativo alcanzado (agrupa nivel de estudios y estado)
df <- df %>% 
  mutate(nivel_educativo = case_when(nivel_de_estudios_alcanzado %in% c("Posgrado", "Posdoctorado", "Doctorado") ~ "Posgrado", 
                                     TRUE ~ nivel_de_estudios_alcanzado), nivel_edu_alcanzado = paste(nivel_educativo, sep = " ", estado))
df
```

### Partición del dataset en train y test

En este caso para evaluar los modelos vamos a realizar una partición entre dataset de entrenamiento (70%) y testeo (30%) usando la función resample_partition del paquete modelr.

```{r}
# fijamos semilla
set.seed(44)
# Partición Train y Test
train_test <- df %>% 
  modelr::resample_partition(c(train = 0.7, test = 0.3))
# armamos dataframe de testeo y entrenamiento
df_train <- train_test$train %>% as_tibble()
df_test <- train_test$test %>% as_tibble()
# vemos el contenido
df_train %>%
  glimpse() # 111 filas
df_test %>%
  glimpse() # 48 filas
```
Habíamos realizado tres modelos distintos para tratar de explicar el salario neto: 

* Modelo **Años de experiencia y Gente a cargo**

* Modelo **Años de experiencia y Género**

* Modelo **Años de experiencia y Nivel Educativo Alcanzado**

Volvemos a realizar estos modelos utilizando el dataset de entrenamiento:

```{r}
# Modelo Experiencia y Gente a cargo
modelo_exp_gc <- lm(salario_neto ~ anos_de_experiencia + gente_a_cargo, data = df_train)
# Modelo Experiencia y Género
modelo_exp_sex <- lm(salario_neto ~ anos_de_experiencia + me_identifico, data = df_train)
# Modelo Experiencia y Nivel Educativo
modelo_exp_edu <- lm(salario_neto ~ anos_de_experiencia + nivel_edu_alcanzado, data = df_train)
# Modelo Experiencia, Género y Gente a cargo
modelo_varias <- lm(salario_neto ~ anos_de_experiencia + gente_a_cargo + me_identifico, data = df_train)
```

En el notebook previo sólo habíamos interpretado el valor de los parámetros estimados y su nivel de significación. Ahora buscaremos responder preguntas tales como:

¿Qué proporción de la variabilidad logra explicar el modelo? ¿Cómo decidir que modelo explica mejor el fenómeno?

¿El modelo cumple con los supuestos del modelo lineal?

## Evaluación del Modelo 

Utilizando el paquete broom, vamos a analizar las medidas de resumen del modelo y graficamos coeficientes estimados. 

**Modelo Años de experiencia y Gente a cargo**

```{r}
# medidas de resumen tidy (incluido el intervalo de confianza)
tidy_meg <- tidy(modelo_exp_gc, conf.int = TRUE)
tidy_meg
# Plot de los Coeficientes
ggplot(tidy_meg, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point(color = "forestgreen") +
  geom_vline(xintercept = 0, lty = 4, color = "black") +
  geom_errorbarh(color = "forestgreen") +
  theme_bw() +
  labs(y = "Coeficientes β", x = "Estimación")
```

En este gráfico podemos observar los coeficientes estimados para este modelo con sus respectivos intervalos de confianza. Tanto el gráfico como la salida del modelo tidy se puede apreciar que el intervalo de confianza (IC) del 95% de la variable años de experiencia no contiene al 0, mientras el IC de la variable gente_a_cargo sí. Es decir, que la experiencia resulta significativa y la gente a cargo no para explicar el salario neto. 

**Modelo Años de experiencia y Género**

```{r}
# medidas de resumen tidy (incluido el intervalo de confianza)
tidy_mes <- tidy(modelo_exp_sex, conf.int = TRUE)
tidy_mes
# Plot de los Coeficientes
ggplot(tidy_mes, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point(color = "forestgreen") +
  geom_vline(xintercept = 0, lty = 4, color = "black") +
  geom_errorbarh(color = "forestgreen") +
  theme_bw() +
  labs(y = "Coeficientes β", x = "Estimación")
```

En este caso se observa que la variable años de experiencia y la categoría Mujer de la variable me_identifico resultan estadísticamente significativas para explicar al sueldo neto y los intervalos de confianza del 95% para ambos coeficientes estimados no contienen al 0.

**Modelo Años de experiencia y Nivel Educativo Alcanzado**

```{r}
# medidas de resumen tidy (incluido el intervalo de confianza)
tidy_meed <- tidy(modelo_exp_edu, conf.int = TRUE)
tidy_meed
# Plot de los Coeficientes
ggplot(tidy_meed, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point(color = "forestgreen") +
  geom_vline(xintercept = 0, lty = 4, color = "black") +
  geom_errorbarh(color = "forestgreen") +
  theme_bw() +
  labs(y = "Coeficientes β", x = "Estimación")
```

Tal como se había observado en la clase 5, vemos que hay algunos IC que contienen al 0 y otros que no. Es decir, que algunos niveles de la variable nivel educativo alcanzado resultan estadísticamente significativos y otros no.  
**Modelo Varias**

```{r}
# medidas de resumen tidy (incluido el intervalo de confianza)
tidy_varias <- tidy(modelo_varias, conf.int = TRUE)
tidy_varias
# Plot de los Coeficientes
ggplot(tidy_varias, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point(color = "forestgreen") +
  geom_vline(xintercept = 0, lty = 4, color = "black") +
  geom_errorbarh(color = "forestgreen") +
  theme_bw() +
  labs(y = "Coeficientes β", x = "Estimación")
```

Podemos calcular las variables resumen para todos los modelos juntos, usando la función map_df de la librerìa purrr para poder mostrar las salidas todas en un mismo dataframe. 

```{r}
# armamos lista con todos los modelos
models <- list(modelo_exp_gc = modelo_exp_gc, modelo_exp_sex = modelo_exp_sex, modelo_exp_edu = modelo_exp_edu, modelo_varias = modelo_varias)
# calculamos las variables resumen
purrr::map_df(models, broom::tidy, .id = "model")
```

### Coeficientes de determinación $R^2$ y $R^2$ ajustado

$$ R^2 = 1 − \frac{SSRes}{SSTot} = \frac{SSReg}{SSTot} $$

El $R^2$ permite medir el porcentaje de variabilidad del fenómeno que el modelo logra explicar. 

Sin importar la relevancia de la/s variables regresoras, el $R^2$ aumenta al agregar una variable adicional al modelo, aunque no se incremente la capacidad explicativa. Es decir, que podríamos utilizarlo para comparar modelos con igual número de variables, pero en casos de modelos con distinto número no serían comparables. En estos casos conviene comparar modelos por medio del $R^2$ ajustado que incluyen justamente el número de variables en el modelo. 

$$ R^2_{a,p} = 1 − \frac{(\frac{SSRes_p}{n − p})}{(\frac{SSTot}{n − 1})} = 1 - (\frac{n-1}{n-p})(\frac{SSRes_p}{SSTot}) $$
Como $(SSTot/n−1)$ está fijo en un conjunto de datos dado (sólo depende de las
Y observadas), el $R_a^2$ aumenta si y sólo si la $SSRes_p$ disminuye.

Veamos qué pasa con los ajustes qué hicimos. 

```{r}
# calculamos las métricas para todos los modelos
purrr::map_df(models, broom::glance, .id = "model") %>%
  # ordenamos por R2 ajustado
  arrange(desc(adj.r.squared))
```

¿Cuál es el modelo que mejor explica la variabildad del conjunto?

Como no todos los modelos tienen igual número de variables, debemos compararlos a través del $R_a^2$, ya que sino podemos incurrir en un error. Por ejemplo, en el caso de modelo_exp_edu el $R^2$ es el mayor de todos pero cuando analizamos el $R_a^2$ es el menor. Esto significa que el $R^2$  de ese modelo solo se incrementó por la cantidad de variables dummies del modelo, no por ser un mejor modelo.

En este caso el mejor modelo resulta ser modelo_exp_sex: que explica el salario neto en función de los años de experiencia y el género. 

### Diagnóstico de Modelos

El diagnóstico del modelo consiste en utilizar técnicas para validar el cumplimiento (o no) de los supuestos del modelo lineal. Recordemos que estos supuestos se puede resumir en:

$ε_i ∼ N(0,σ^2)$ independientes entre sí.

Los errores tienen distribución normal con media cero y varianza constante y son independientes entre sí. Los errores son inobservables, por lo tanto tendremos que trabajar con su correlato empírico: los residuos en las técnicas de diagnóstico.

```{r}
plot(modelo_exp_sex)
```
* *Residuos vs valores predichos*: Parece existir cierta estructura en los datos: la varianza parece incrementarse con los valores predichos (depende de la variable) por lo que no se satisface el supuesto de homocedasticidad.

* *Normal QQ plot*: El extremo superior derecho no se ajusta a la distribución teórica.

* *Residual vs leverage*: Existen dos puntos con un leverage bastante alto, el gráfico destaca la observación 40.

```{r}
df_train[40,]
```

* *Diagnóstico del modelo*: El modelo creado no cumple con los supuestos del modelo lineal. Parecen existir dos problemas: la existencia de heterocedasticidad (varianza no constante) y la presencia de observaciones de alto leverage. 

Veamos la versión tidy de estos gráficos. 

```{r}
# calculamos valores predichos para todos los modelos
au_modelos = purrr::map_df(models, broom::augment, .id = "model")
au_modelos %>%
  head()
```
Al contar con esta información como dataframe, podemos realizar los gráficos de diagnóstico con ggplot. Vamos a ver dos ejemplos con los mejores modelos, pero se podría hacer para todos el mismo ejercicio. 

```{r}
# Modelo experiencia y género
g1 = ggplot(au_modelos %>% filter(model == "modelo_exp_sex"), 
       aes(.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(se = FALSE) +
  labs(title = "Residuos vs valores predichos") + 
  theme_bw()
g2 = ggplot(au_modelos %>% filter(model == "modelo_exp_sex"), 
       aes(sample = .std.resid)) +
  stat_qq() +
  geom_abline() +
  labs(title = "Normal QQ plot") + 
  theme_bw()
g3 = ggplot(au_modelos %>% filter(model == "modelo_exp_sex"), 
       aes(.fitted, sqrt(abs(.std.resid)))) +
  geom_point() +
  geom_smooth(se = FALSE) + 
  theme_bw() +
  labs(title = "Scale-location plot")
g4 = ggplot(au_modelos %>% filter(model == "modelo_exp_sex"), 
       aes(.hat, .std.resid)) +
  geom_vline(size = 2, colour = "white", xintercept = 0) +
  geom_hline(size = 2, colour = "white", yintercept = 0) +
  geom_point() + 
  geom_smooth(se = FALSE) + 
  theme_bw() +
  labs(title = "Residual vs leverage")
# grafico todos juntos
grid.arrange(g1, g2, g3, g4, nrow = 2)
```

```{r}
# Modelo varias
g1 = ggplot(au_modelos %>% filter(model ==  "modelo_varias"), 
       aes(.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(se = FALSE) +
  labs(title = "Residuos vs valores predichos") + 
  theme_bw()
g2 = ggplot(au_modelos %>% filter(model == "modelo_varias"), 
       aes(sample = .std.resid)) +
  stat_qq() +
  geom_abline() +
  labs(title = "Normal QQ plot") + 
  theme_bw()
g3 = ggplot(au_modelos %>% filter(model == "modelo_varias"), 
       aes(.fitted, sqrt(abs(.std.resid)))) +
  geom_point() +
  geom_smooth(se = FALSE) + 
  theme_bw() +
  labs(title = "Scale-location plot")
g4 = ggplot(au_modelos %>% filter(model == "modelo_varias"), 
       aes(.hat, .std.resid)) +
  geom_vline(size = 2, colour = "white", xintercept = 0) +
  geom_hline(size = 2, colour = "white", yintercept = 0) +
  geom_point() + 
  geom_smooth(se = FALSE) + 
  theme_bw() +
  labs(title = "Residual vs leverage")
# grafico todos juntos
grid.arrange(g1, g2, g3, g4, nrow = 2)
```

1) medidas de evaluación  de R cuadrado ajustado
2) medidas de predicción
3) una discusión más general de explicación VS predicción
